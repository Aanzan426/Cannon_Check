{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca45995-bbf9-45a7-a4b7-8b574d0c5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NLTK setup (must be first cell in every notebook) ---\n",
    "import nltk\n",
    "\n",
    "NLTK_DATA_DIR = \"/ml-envs/kharagpur_env/nltk_data\"\n",
    "if NLTK_DATA_DIR not in nltk.data.path:\n",
    "    nltk.data.path.append(NLTK_DATA_DIR)\n",
    "\n",
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pathway as pw\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "TRAIN_PATH = \"/media/deepaansh-sial/c7e4e807-62df-49d6-a165-9797ceac5277/IIT Kharagpur Data Science Hackathon/train.csv\"\n",
    "TEST_PATH  = \"/media/deepaansh-sial/c7e4e807-62df-49d6-a165-9797ceac5277/IIT Kharagpur Data Science Hackathon/test.csv\"\n",
    "\n",
    "BOOK_FILES = {\n",
    "    \"In Search of the Castaways\": \"/media/deepaansh-sial/c7e4e807-62df-49d6-a165-9797ceac5277/IIT Kharagpur Data Science Hackathon/Books/In search of the castaways.txt\",\n",
    "    \"The Count of Monte Cristo\": \"/media/deepaansh-sial/c7e4e807-62df-49d6-a165-9797ceac5277/IIT Kharagpur Data Science Hackathon/Books/The Count of Monte Cristo.txt\",\n",
    "}\n",
    "\n",
    "TOP_K_EVIDENCE = 2  # LOCKED\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "train_df[\"label\"] = train_df[\"label\"].str.lower()\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Test:\", test_df.shape)\n",
    "\n",
    "# =========================\n",
    "# Book loading & chunking\n",
    "# =========================\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def load_book_text(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read().replace(\"\\r\", \" \")\n",
    "\n",
    "def chunk_text(text, max_tokens=700, overlap_tokens=100):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks, current, tokens = [], [], 0\n",
    "\n",
    "    for s in sentences:\n",
    "        s_tokens = int(len(s.split()) * 1.3)\n",
    "        if tokens + s_tokens > max_tokens:\n",
    "            chunks.append(\" \".join(current))\n",
    "            overlap, used = [], 0\n",
    "            for sent in reversed(current):\n",
    "                t = int(len(sent.split()) * 1.3)\n",
    "                overlap.insert(0, sent)\n",
    "                used += t\n",
    "                if used >= overlap_tokens:\n",
    "                    break\n",
    "            current, tokens = overlap, used\n",
    "\n",
    "        current.append(s)\n",
    "        tokens += s_tokens\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "    return chunks\n",
    "\n",
    "book_chunks = {}\n",
    "for book, path in BOOK_FILES.items():\n",
    "    text = load_book_text(path)\n",
    "    chunks = chunk_text(text)\n",
    "    book_chunks[book] = [\n",
    "        {\"book\": book, \"chunk_id\": i, \"text\": c}\n",
    "        for i, c in enumerate(chunks)\n",
    "    ]\n",
    "    print(book, \"chunks:\", len(chunks))\n",
    "\n",
    "# =========================\n",
    "# Embeddings + Pathway table\n",
    "# =========================\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "@pw.udf\n",
    "def embed_text(text: str):\n",
    "    return embedder.encode(text).tolist()\n",
    "\n",
    "def build_pathway_table(book_chunks):\n",
    "    records = []\n",
    "    for book, chunks in book_chunks.items():\n",
    "        for c in chunks:\n",
    "            records.append({\n",
    "                \"book\": c[\"book\"],\n",
    "                \"chunk_id\": c[\"chunk_id\"],\n",
    "                \"text\": c[\"text\"]\n",
    "            })\n",
    "    return pw.debug.table_from_pandas(pd.DataFrame(records))\n",
    "\n",
    "novel_table = build_pathway_table(book_chunks)\n",
    "\n",
    "novel_table = novel_table.select(\n",
    "    book=pw.this.book,\n",
    "    chunk_id=pw.this.chunk_id,\n",
    "    text=pw.this.text,\n",
    "    embedding=embed_text(pw.this.text)\n",
    ")\n",
    "\n",
    "novel_df = pw.debug.table_to_pandas(novel_table)\n",
    "novel_df[\"embedding\"] = novel_df[\"embedding\"].apply(np.array)\n",
    "\n",
    "# =========================\n",
    "# Retrieval\n",
    "# =========================\n",
    "def retrieve_evidence(book_name, queries):\n",
    "    book_df = novel_df[novel_df[\"book\"] == book_name].reset_index(drop=True)\n",
    "    if len(book_df) == 0:\n",
    "        return [{\"evidence\": []} for _ in queries]\n",
    "\n",
    "    book_embeddings = np.vstack(book_df[\"embedding\"].values)\n",
    "    knn = NearestNeighbors(\n",
    "        n_neighbors=min(TOP_K_EVIDENCE, len(book_df)),\n",
    "        metric=\"cosine\"\n",
    "    )\n",
    "    knn.fit(book_embeddings)\n",
    "\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        q_emb = embedder.encode(q).reshape(1, -1)\n",
    "        _, idxs = knn.kneighbors(q_emb)\n",
    "        evidence = [{\"text\": book_df.iloc[i][\"text\"]} for i in idxs[0]]\n",
    "        results.append({\"evidence\": evidence})\n",
    "\n",
    "    return results\n",
    "\n",
    "# =========================\n",
    "# Feature extraction (FINAL)\n",
    "# =========================\n",
    "def extract_features(backstory, book_name):\n",
    "    evidence = retrieve_evidence(book_name, [backstory])[0][\"evidence\"]\n",
    "\n",
    "    back_emb = embedder.encode(backstory).reshape(1, -1)\n",
    "    ev_embs = np.vstack([embedder.encode(ev[\"text\"]) for ev in evidence])\n",
    "    sims = cosine_similarity(back_emb, ev_embs)[0]\n",
    "\n",
    "    book_df = novel_df[novel_df[\"book\"] == book_name]\n",
    "    book_embs = np.vstack(book_df[\"embedding\"].values)\n",
    "    book_sims = cosine_similarity(back_emb, book_embs)[0]\n",
    "\n",
    "    return {\n",
    "        \"max_sim\": float(np.max(sims)),\n",
    "        \"mean_sim\": float(np.mean(sims)),\n",
    "        \"sim_percentile\": float((book_sims < np.max(sims)).mean()),\n",
    "        \"sim_zscore\": float(\n",
    "            (np.max(sims) - book_sims.mean()) / (book_sims.std() + 1e-6)\n",
    "        ),\n",
    "        \"backstory_len\": len(backstory.split()),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Build feature dataset\n",
    "# =========================\n",
    "feature_rows = []\n",
    "for _, row in train_df.iterrows():\n",
    "    feats = extract_features(row[\"content\"], row[\"book_name\"])\n",
    "    feats[\"label\"] = 1 if row[\"label\"] == \"consistent\" else 0\n",
    "    feature_rows.append(feats)\n",
    "\n",
    "feature_df = pd.DataFrame(feature_rows)\n",
    "\n",
    "# =========================\n",
    "# Train / validation\n",
    "# =========================\n",
    "X = feature_df.drop(columns=[\"label\"])\n",
    "y = feature_df[\"label\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, clf.predict(X_val)))\n",
    "\n",
    "# =========================\n",
    "# Final training + submission\n",
    "# =========================\n",
    "clf.fit(X, y)\n",
    "\n",
    "test_features = []\n",
    "for _, row in test_df.iterrows():\n",
    "    test_features.append(\n",
    "        extract_features(row[\"content\"], row[\"book_name\"])\n",
    "    )\n",
    "\n",
    "X_test = pd.DataFrame(test_features)\n",
    "test_preds = clf.predict(X_test)\n",
    "\n",
    "test_df[\"label\"] = np.where(test_preds == 1, \"consistent\", \"contradict\")\n",
    "test_df[[\"id\", \"label\"]].to_csv(\"result.csv\", index=False)\n",
    "\n",
    "print(\"result.csv generated âœ…\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kharagpur Hackathon-Pipeline)",
   "language": "python",
   "name": "khargapur_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
